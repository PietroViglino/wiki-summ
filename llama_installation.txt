installare NVIDIA CUDA Toolkit

per verificare se l'installazione è avvenuta correttamente:
    nvcc.exe -V

dal terminale (powershell):
    $env:set="FORCE_CMAKE=1"
    $env:CMAKE_ARGS="-DLLAMA_CUBLAS=on"     
    pip install llama-cpp-python --force-reinstall --upgrade --no-cache-dir --verbose  

per essere sicuri runnare questo codice:

    from llama_cpp import Llama
    llm = Llama(model_path="LLM/llama-2-7b-chat.Q4_K_M.gguf", n_gpu_layers=30, n_ctx=3584, n_batch=521, verbose=True)
    # adjust n_gpu_layers as per your GPU and model
    output = llm("Q: Name the planets in the solar system? A: ", max_tokens=0, stop=["Q:", "\n"], echo=True)
    print(output)

 nel terminale ci sarà:
 AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 |    
Model metadata: {'general.name': 'LLaMA v2', 'general.architecture': 'llama', 'llama.context_length': '4096', 'llama.rope.dimension_count': '128', 'llama.embedding_length': '4096', 'llama.block_count': '32', 
'llama.feed_forward_length': '11008', 'llama.attention.head_count': '32', 'tokenizer.ggml.eos_token_id': '2', 'general.file_type': '15', 'llama.attention.head_count_kv': '32', 'llama.attention.layer_norm_rms_epsilon': '0.000001',
 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '0'}

BLAS = 1  significa che la computazione è stata assegnata con successo alla GPU

LINK UTILI(zzati):

https://pypi.org/project/llama-cpp-python/  -- installazione BLAS
https://huggingface.co/TheBloke    -- Download modelli